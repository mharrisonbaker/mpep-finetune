# MPEP LLaMA Fine-Tuning

This repository contains scripts and data for fine-tuning the LLaMA 3.1 8B model on legal texts from the **Manual of Patent Examining Procedure (MPEP)**.

## ğŸš€ Project Goals
- Improve LLaMA's ability to answer patent law-related questions.
- Train on structured **question-answer (QA) pairs** extracted from the MPEP.
- Utilize **LoRA/QLoRA** to efficiently fine-tune on a GPU.

## ğŸ“‚ Repository Structure
```
ğŸ“‚ mpep-llama-finetune
 â”œâ”€â”€ notebooks/                # Jupyter notebooks for preprocessing and analysis
 â”œâ”€â”€ scripts/                  # Python scripts for fine-tuning and dataset processing
 â”œâ”€â”€ datasets/                 # Training data (ignored in GitHub)
 â”œâ”€â”€ models/                   # Checkpoints (ignored in GitHub)
 â”œâ”€â”€ config.json               # Model training configuration
 â”œâ”€â”€ .gitignore                # Files to exclude from Git tracking
 â”œâ”€â”€ README.md                 # Project documentation
```

## ğŸ› ï¸ Setup
### 1ï¸âƒ£ Install Dependencies
```sh
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run Preprocessing
```sh
python scripts/process_mpep_json.py
```

### 3ï¸âƒ£ Start Fine-Tuning
```sh
python scripts/finetune_llama3.py
```

## ğŸ“Š Expected Results
- The fine-tuned model should better understand **patent law terminology**.
- It should answer **MPEP-related questions** with higher accuracy.

## ğŸ“Œ TODO
- âœ… Add dataset processing scripts
- âœ… Implement LoRA fine-tuning
- ğŸš€ Optimize model inference for fast response times

## ğŸ¤ Contributing
Feel free to submit issues or pull requests if you have improvements!

## ğŸ“œ License
This project is open-source under the **MIT License**.
